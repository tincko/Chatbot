# âš¡ GuÃ­a de Inicio RÃ¡pido

Â¿Quieres probar la aplicaciÃ³n **ahora mismo**? Sigue estos pasos.

## ğŸ“¦ Prerrequisitos (5 minutos)

### 1. Instala Ollama
```bash
# Windows: Descarga desde https://ollama.ai/download/windows
# Mac: 
brew install ollama
# Linux:
curl https://ollama.ai/install.sh | sh
```

### 2. Descarga un modelo LLM
```bash
# Modelo ligero (recomendado para empezar)
ollama pull llama3.2:3b

# O modelo de calidad media
ollama pull qwen2.5:7b
```

### 3. Verifica Python y Node.js
```bash
python --version  # Debe ser 3.8+
node --version    # Debe ser 16+
```

Si no los tienes:
- **Python**: https://www.python.org/downloads/
- **Node.js**: https://nodejs.org/

## ğŸš€ InstalaciÃ³n (5 minutos)

### OpciÃ³n A: Script AutomÃ¡tico (MÃ¡s FÃ¡cil)

**Windows:**
```bash
git clone https://github.com/tuusuario/chatbot-terapia.git
cd chatbot-terapia
start_app.bat
```

**Mac/Linux:**
```bash
git clone https://github.com/tuusuario/chatbot-terapia.git
cd chatbot-terapia
chmod +x start_app.sh
./start_app.sh
```

El script:
âœ… Crea el entorno virtual de Python  
âœ… Instala todas las dependencias  
âœ… Inicia backend y frontend automÃ¡ticamente

### OpciÃ³n B: Manual

**1. Backend:**
```bash
cd web_app/backend
python -m venv venv

# Windows:
venv\Scripts\activate
# Mac/Linux:
source venv/bin/activate

pip install -r requirements.txt
python main.py
```

**2. Frontend (en otra terminal):**
```bash
cd web_app/frontend
npm install
npm run dev
```

## ğŸ® Primer Uso (5 minutos)

### 1. Abre la aplicaciÃ³n
Navega a: **http://localhost:5173**

### 2. Configura los modelos (PestaÃ±a "Setup")
- **Psychologist Model**: Selecciona `qwen2.5:7b` o el modelo que instalaste
- **Patient Helper Model**: Selecciona el mismo u otro modelo
- Click en "Load Default Prompts"

### 3. Genera un paciente
- Scroll down â†’ "Patient Profile"
- Click en **"Generate with AI"**
- Espera ~30 segundos
- Revisa el perfil generado
- Click en **"Save Patients"**

### 4. Inicia una sesiÃ³n (PestaÃ±a "Chat")
- El psicÃ³logo (IA) te saludarÃ¡ automÃ¡ticamente
- Escribe tu respuesta como terapeuta
- Click en "Get Suggestion" si quieres una sugerencia del paciente
- ContinÃºa la conversaciÃ³n
- Click en "Save Chat" cuando termines

### 5. Analiza la sesiÃ³n (PestaÃ±a "History")
- VerÃ¡s tu sesiÃ³n guardada
- SelecciÃ³nala con el checkbox
- Click en **"Analyze with AI"**
- Espera el anÃ¡lisis (~1 minuto)
- Lee el anÃ¡lisis generado
- Opcionalmente, exporta a PDF

## âœ… Â¡Listo!

Ya tienes el sistema funcionando. Ahora puedes:

- ğŸ‘¥ Crear mÃ¡s pacientes
- ğŸ’¬ Practicar diferentes escenarios terapÃ©ticos
- ğŸ“Š Analizar tus sesiones
- ğŸ“š Subir documentos de referencia (RAG)
- ğŸ¤– Generar sesiones automÃ¡ticas

## ğŸ†˜ Problemas Comunes

### "Error: Connection refused"
**SoluciÃ³n**: El backend no estÃ¡ corriendo. Verifica que `python main.py` estÃ© ejecutÃ¡ndose en otra terminal.

### "Model not found"
**SoluciÃ³n**: 
```bash
ollama pull qwen2.5:7b
```

### El LLM responde muy lento
**SoluciÃ³n**: 
- Usa un modelo mÃ¡s pequeÃ±o: `ollama pull llama3.2:3b`
- Reduce `max_tokens` en Setup (ej: 300)
- Cierra otras aplicaciones

### "Module not found: FastAPI"
**SoluciÃ³n**:
```bash
cd web_app/backend
# Activa el venv primero
pip install -r requirements.txt
```

## ğŸ“š Siguiente Paso

Una vez que domines lo bÃ¡sico:

1. Lee el [README completo](README.md) para caracterÃ­sticas avanzadas
2. Consulta [FAQ.md](FAQ.md) para preguntas comunes
3. Revisa [ARCHITECTURE.md](ARCHITECTURE.md) si quieres contribuir

## ğŸ’¡ Tips

- **Modelos recomendados por hardware**:
  - 8GB RAM: `llama3.2:3b`
  - 16GB RAM: `qwen2.5:7b` o `llama3.1:8b`
  - 32GB+ RAM o GPU: `llama3.1:70b` o superior

- **Prompts mÃ¡s efectivos**:
  - SÃ© especÃ­fico con el contexto del paciente
  - Incluye el enfoque terapÃ©utico deseado (CBT, humanista, etc.)
  - Menciona objetivos de la sesiÃ³n

- **RAG mÃ¡s Ãºtil**:
  - Sube guÃ­as clÃ­nicas relevantes
  - Incluye protocolos de tratamiento
  - Agrega literatura especializada

---

**Â¿Todo funcionando?** Â¡Excelente! Ahora a practicar ğŸ¯

**Â¿Problemas?** Consulta [FAQ.md](FAQ.md) o abre un issue en GitHub.
